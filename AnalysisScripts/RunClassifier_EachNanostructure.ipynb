{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from scipy import stats\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, auc\n",
    "\n",
    "outputDir = 'Results\\\\'\n",
    "\n",
    "dataPresentPath=sys.argv[1]\n",
    "dataPresent=pd.read_csv(dataPresentPath)\n",
    "\n",
    "overallRes3 = dataPresent[['Present', 'Sample', 'Protein']]\n",
    "overallResPres=overallResPres.rename(columns={\"Protein\":'ID'})\n",
    "overallResPres=overallResPres.rename(columns={\"Present\":'Abundance'})\n",
    "dataPresent.drop(['Present', 'Protein', 'Sample', 'Unnamed: 0'],inplace = True, axis =1)\n",
    "\n",
    "classifier = xgb.XGBClassifier()\n",
    "structs = list()\n",
    "f1_all = list()\n",
    "acc_structs = list()\n",
    "prec_all = list()\n",
    "rec_all = list()\n",
    "allScores = list()\n",
    "featureimportancesTot = list()\n",
    "allAUCs = list()\n",
    "for z in np.unique(overallRes3['Sample']):\n",
    "    subRes = overallRes3[overallRes3['Sample'] == z]\n",
    "    abundanceTest1 = subRes['Abundance']\n",
    "    subX=dataNew.loc[subRes.index]\n",
    "    subX = subX.reset_index(drop = True)\n",
    "    subY= np.asarray(abundanceTest1)\n",
    "    #pros = list()\n",
    "    #for x in range(len(abundanceBin)):\n",
    "    #    bool1=abundanceBin[x]\n",
    "    #    if(bool1 == 1):\n",
    "    #        pros.append(subRes.loc[x,'Proteins'])\n",
    "    #pd.DataFrame(pros).to_csv(z + '_PresentPros.csv', index = None, header = None)\n",
    "    kFold=KFold(n_splits=10,shuffle=True, random_state=42)\n",
    "    clf = classifier\n",
    "\n",
    "    features = list()\n",
    "    scores = list()\n",
    "    f1_structs = list()\n",
    "    prec_structs = list()\n",
    "    rec_structs = list()\n",
    "    aucScores = list()\n",
    "    for train_index,test_index in kFold.split(subX):\n",
    "        X_train, X_test, y_train, y_test = subX.loc[train_index], subX.loc[test_index], subY[train_index], subY[test_index]\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        f1_structs.append(metrics.f1_score(y_test, y_pred))\n",
    "        prec_structs.append(metrics.precision_score(y_test, y_pred))\n",
    "        rec_structs.append(metrics.recall_score(y_test, y_pred))\n",
    "        features.append(clf.feature_importances_)\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        aucScores.append(roc_auc_score(y_test,y_pred[:,1]))\n",
    "        structs.append(z)\n",
    "    #explainer = shap.TreeExplainer(clf, X_train)\n",
    "    #shap_values = explainer.shap_values(X_test)\n",
    "    #plt.figure()\n",
    "    #plt.title(z)\n",
    "    #shap.summary_plot(shap_values, X_test.astype(\"float\"), show=False)\n",
    "    #plt.savefig(z + ' SHAP.png')\n",
    "    allScores.append(scores)\n",
    "    allAUCs.append(aucScores)\n",
    "    f1_all.append(f1_structs)\n",
    "    featureimportancesTot.append(features)\n",
    "    prec_all.append(prec_structs)\n",
    "    rec_all.append(rec_structs)\n",
    "\n",
    "allStructsMet=pd.DataFrame({'Structures':structs,'AUC':np.array(allAUCs).flatten(),'F1':np.array(f1_all).flatten(),'Accuracy':np.array(allScores).flatten(),'Precision':np.array(prec_all).flatten(),'Recall':np.array(rec_all).flatten()})\n",
    "\n",
    "allStructsMet=pd.DataFrame({'Structures':structs,'AUC':np.array(allAUCs).flatten(),'F1':np.array(f1_all).flatten(),'Accuracy':np.array(allScores).flatten(),'Precision':np.array(prec_all).flatten(),'Recall':np.array(rec_all).flatten()})\n",
    "\n",
    "vals = list()\n",
    "metricname  = list()\n",
    "DataSet = list()\n",
    "for i in allStructsMet.columns[1:6]:\n",
    "    vals1=allStructsMet[i]\n",
    "    for z in range(len(vals1)):\n",
    "        DataSet.append(allStructsMet.loc[z,'Structures'])\n",
    "        vals.append(vals1[z])\n",
    "        metricname.append(i)\n",
    "\n",
    "aucVal = list()\n",
    "f1Val = list()\n",
    "accVal = list()\n",
    "precVal = list()\n",
    "recVal = list()\n",
    "for z in np.unique(allStructsMet['Structures']):\n",
    "    structDF=allStructsMet[allStructsMet['Structures'] == z]\n",
    "    f1Val.append(np.mean(structDF['F1']))\n",
    "    accVal.append(np.mean(structDF['Accuracy']))\n",
    "    precVal.append(np.mean(structDF['Precision']))\n",
    "    aucVal.append(np.mean(structDF['AUC']))\n",
    "    recVal.append(np.mean(structDF['Recall']))\n",
    "\n",
    "allStructsMet=pd.DataFrame({'Structures':np.unique(allStructsMet['Structures']),'AUC':aucVal,'F1':f1Val,'Accuracy':accVal,'Precision':precVal,'Recall':recVal})\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', family='serif')\n",
    "#plt.rc('xtick', labelsize='large')\n",
    "#plt.rc('ytick', labelsize='large')\n",
    "\n",
    "#vals = list()\n",
    "#metricname  = list()\n",
    "#DataSet = list()\n",
    "#for i in allStructsMet.columns[1:6]:\n",
    "#    vals1=allmet[i]\n",
    "#    for z in range(len(vals1)):\n",
    "#        DataSet.append(allStructsMet.loc[z,'Structures'])\n",
    "#        vals.append(vals1[z])\n",
    "#        metricname.append(i)\n",
    "everymet=pd.DataFrame({'Scores':vals, 'Metric':metricname, 'Structure':DataSet})\n",
    "\n",
    "#met = allmet\n",
    "labs=[\"{:.2f}\".format(np.mean(allStructsMet['AUC'])),\"{:.2f}\".format(np.mean(allStructsMet['F1'])),\"{:.2f}\".format(np.mean(allStructsMet['Accuracy'])),\"{:.2f}\".format(np.mean(allStructsMet['Recall'])),\"{:.2f}\".format(np.mean(allStructsMet['Precision']))]\n",
    "fig, ax = plt.subplots()\n",
    "g=sns.barplot(data = everymet,x='Metric', y = 'Scores',palette='colorblind', saturation = .7)\n",
    "ax.set_ylim(0, 1)\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,labels = labs,padding=-30)\n",
    "#plt.title('All Information Enriched vs Not Enriched')\n",
    "g.set(ylim=(0, 1), xlabel =\"Metrics\", ylabel = \"Scores\", title = 'Per Nanostructure Performance')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.savefig(outputDir + 'PerNanostructurePerformance.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
